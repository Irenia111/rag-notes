# 多模态嵌入 笔记

## 测试代码
Visualized‑BGE 是由北京智源研究院（BAAI）推出的通用多模态嵌入模型，它在 BGE 文本嵌入架构上融入了图像 token 嵌入，实现了对图像、文本、图文组合的统一编码能力  ￼。

它主要用于三类任务：
- 跨模态知识检索（文本查询，候选为图像、文本或混合）
- 组合图像检索（图像＋文本查询，目标为图像）
- 多模态查询知识检索（图文查询，候选为文本）

它保持了原有 BGE 对文本嵌入的强大能力，同时添加图像处理能力 。
```Python
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
from visual_bge.visual_bge.visual_bge.modeling import Visualized_BGE

model = Visualized_BGE(model_name_bge="BAAI/bge-base-en-v1.5",
                      model_weight="../../models/bge/Visualized_base_en_v1.5.pth")
model.eval()

with torch.no_grad():
    text_emb = model.encode(text="datawhale开源组织的logo")
    img_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png")
    multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="datawhale开源组织的logo")
    img_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png")
    multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png", text="datawhale开源组织的logo")

# 计算相似度
sim_1 = img_emb_1 @ img_emb_2.T
sim_2 = img_emb_1 @ multi_emb_1.T
sim_3 = text_emb @ multi_emb_1.T
sim_4 = multi_emb_1 @ multi_emb_2.T

print("=== 相似度计算结果 ===")
print(f"纯图像 vs 纯图像: {sim_1}")
print(f"图文结合1 vs 纯图像: {sim_2}")
print(f"图文结合1 vs 纯文本: {sim_3}")
print(f"图文结合1 vs 图文结合2: {sim_4}")
```


输出结果：
```text
tokenizer.json: 711kB [00:00, 193MB/s]
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.8291]])
图文结合1 vs 纯文本: tensor([[0.7627]])
图文结合1 vs 图文结合2: tensor([[0.9058]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 0.0360, -0.0032, -0.0377,  0.0240,  0.0140,  0.0340,  0.0148,  0.0292,
         0.0060, -0.0145])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
```


### 相似度基础概念
@ 是向量点积；如果向量做了 L2 归一化，点积就等于余弦相似度（cosine）。在工业里，检索普遍用余弦/内积；FAISS 也说明“cosine 就是对归一化后的内积”。 ￼
- BGE 官方也说默认用余弦相似度做检索（是否在代码里归一化对“检索相似度”不影响）。 ￼
- Visualized‑BGE 的机制：把图像 token接入 BGE 文本编码框架，得到一个“统一嵌入”，用于多模态检索（图↔文↔图文）。这就是你现在能直接拿不同模态算相似度的原因。 ￼

小建议：稳妥起见可以对输出明确做一次 F.normalize(x, dim=1) 再算相似度，和社区/向量库对齐。 ￼ ￼

### 结果分析
- sim_1（图↔图）：以图搜图的能力；0.83 说明两图很像。
- sim_2（图↔图文）：以图搜图文条目的能力；数值很高表示“图文融合几乎等于这张图”。
- sim_3（文↔图文）：以文搜图文条目的能力；能用，但在“同图”场景里文本影响弱于图像。
- sim_4（图文↔图文）：组合检索能力；同图不同词时很高，但会被文本拉开一点点。

### 指标分析 
sim_1：img_emb_1 @ img_emb_2.T（图 ↔ 图）
两张 Datawhale 相关图像相似度 ~0.83，说明是同类/相似但不是完全重复。
▶️ 对应的召回能力：以图搜图（image→image）。把图库里都存成图像嵌入，用上传图片作为查询嵌入，按 cos 排序返回最像的图片。这是最直观也最稳定的一类检索。

sim_2：img_emb_1 @ multi_emb_1.T（图 ↔ 图文融合）
数值很高（~0.97），表示图文融合向量几乎“就是这张图”的语义，文本只是微调。
▶️ 对应的召回能力：以图搜“图文条目”（image→(image+text)）。例如商品目录里每个条目都存成“图+标题/描述”的融合向量，用户传一张图，就能把那条商品召回。Visualized‑BGE 的模型卡就把这种“混合模态检索/组合检索”列为核心应用。 ￼

sim_3：text_emb @ multi_emb_1.T（文 ↔ 图文融合）
0.76（中文提示时）或 ~0.48（英文 “whale” 时），明显低于 sim_2，说明在“同图”场景里图像信号更强，文本对融合向量是“加条件”的作用。
▶️ 对应的召回能力：以文搜“图文条目”（text→(image+text)）。例如你写“whale logo”，从素材库（每条是图+标题）里搜到相关条目。这是做多模态 RAG 时最常用的一种召回：用户只给文字，但底库是带图的知识项。 ￼

sim_4：multi_emb_1 @ multi_emb_2.T（图文 ↔ 图文）
~0.91–0.94：同一张图+相近文本（“whale” vs “blue whale”）→ 很像但不完全相同。
▶️ 对应的召回能力：以“图+文”搜“图+文”（组合检索，image+text→image+text）。例如“这张 logo，但要蓝色主题”，就用（图+提示词）作查询，去找库里（图+描述）最匹配的结果。模型卡把它叫 Composed Image Retrieval。 

## 修改参数
### 修改文本
```python
multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="鲸鱼")
multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png", text="蓝鲸")
```

结果
```text
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.9926]])
图文结合1 vs 纯文本: tensor([[0.5061]])
图文结合1 vs 图文结合2: tensor([[0.8323]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 4.7757e-02, -5.5364e-02,  5.0497e-03,  1.5420e-05,  2.4755e-02,
         3.8132e-02,  1.7600e-02,  3.6535e-02, -3.9895e-02, -2.3314e-02])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
        -0.0380, -0.0270])
```

图文结合1 vs 纯图像: tensor([[0.9926]])
图文结合1 vs 纯文本: tensor([[0.5061]])
图文结合1 vs 图文结合2: tensor([[0.8323]])
这三个指标变化很明显, 猜测文本提示（text prompt）对嵌入相似度影响非常大

1.	文本 prompt 极大影响组合嵌入（multi_emb）
当添加了“鲸鱼”（修改前），模型生成的图文组合嵌入与纯图像嵌入仍保持很高的相似度（0.8291），说明图像信息主导；但添加“datawhale 开源组织的 logo”时，模型关注了组织 logo 的语义、图像中与 logo 匹配度更高的部分，导致相似度跃升至 0.9926，几乎一模一样。
2.	图文 vs 纯文本的距离急剧变化
修改后“图文结合1 vs 纯文本”相似度从 0.7627 跳降到 0.5061，说明使用“鲸鱼”（相比原提示更偏图像）使得“图文嵌入”与仅有文本的嵌入差别更大，模型更依赖图像内容，而不是文本。
3.	两个图文提示之间的关系变动
“图文结合1 vs 图文结合2”原为 0.9058（高），修改后仅为 0.8323，说明“鲸鱼”与“蓝鲸”提示虽然相似，但模型更加敏感于文本差异了。显然，提示词语的选择使得组合嵌入之间距离拉大。


### 使用相同图片，但是文本不同
```python
multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="鲸鱼")
multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="蓝鲸")
```

```text
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.9926]])
图文结合1 vs 纯文本: tensor([[0.5061]])
图文结合1 vs 图文结合2: tensor([[1.0000]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 4.7757e-02, -5.5364e-02,  5.0497e-03,  1.5420e-05,  2.4755e-02,
         3.8132e-02,  1.7600e-02,  3.6535e-02, -3.9895e-02, -2.3314e-02])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
        -0.0380, -0.0270])
```

图文结合1 vs 图文结合2 的相似度直接变成 1.0000，意味着两个向量几乎完全相同

可能得原因：
- 同图强主导 + 归一化
Visualized‑BGE 的相似度通常是内积/余弦相似度；向量会做归一化。若“图文嵌入”在实现里由图像分支主导（或文本权重很小），那在同一张图的情况下，哪怕文本略有不同，最后融合后的向量也会基本一样，于是同图的两次图文编码彼此相似度≈1。 官方用法也把它定位在“组合图像检索”等场景，图像信号会很强，这是正常现象。 ￼

- 当前底座是英文模型，中文词信号可能被弱化
当前使用的底座是 BAAI/bge-base-en-v1.5（英文），而提示词是中文“鲸鱼/蓝鲸”。英文底座对中文的语义覆盖有限，文本分支贡献度更小，就更容易出现“同图导致同向量”的情况。相比之下，官方也提供了多语种的 bge-m3 版本，并在模型卡里直接演示了用中文文本进行图文组合检索的例子（"一匹马牵着这辆车"）。若换成 Visualized‑M3，多语言文本的影响会明显增强。
￼
- 另一条证据：图文 vs 纯图像 ≈ 0.9926
同时看到 “图文结合1 vs 纯图像” 的相似度 ≈ 0.9926，这进一步说明该图文嵌入几乎就是这张图的嵌入（文本只带来微小扰动）。这与上面两点完全吻合。官方示例里也采用“图文作查询，纯图作候选”的内积比较，属于同一评价范式。 ￼


### 使用英文文本，相同图片
```python
multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="whale")
multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="blue whale")
```

```text
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.9713]])
图文结合1 vs 纯文本: tensor([[0.4838]])
图文结合1 vs 图文结合2: tensor([[0.9441]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 0.0293, -0.0514, -0.0218,  0.0101,  0.0242,  0.0251,  0.0122,  0.0402,
        -0.0490, -0.0197])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
        -0.0380, -0.0270])
```

为什么这次英文提示相比中文略微“增强”了文本影响？
- 使用英文（例如 “whale” vs “blue whale”）让文本编码器更“熟悉”这些词，这带来：
- 图文 vs 图文的相似度（0.9441）稍低于之前中文近 1.0 的情况，说明文本确实在拉开差距；
- 图文 vs 图像相似度从 ≈ 0.99 降到 0.97，也说明文本带来更多影响。
- 英文文本对英文模型更有效，但文本整体仍然被图像强信号覆盖，因此效果提升有限，但方向明确。


总结判断:
- **没有显式的“图像 vs 文本权重”** 可调整，融合是由模型结构与训练决定的；
- 图像路径输入通常对最终嵌入影响最强，特别是当文本输入语言（如中文）与底座模型不匹配时；
- 使用更匹配的语言（如英文）会在一定程度上增强文本影响，但仍是“弱影响”；
- 若使用 多语模型（如 bge-m3），文本影响会更显著，尤其对中文或复合文本输入。

## 新手避坑
- 看排序，不迷信绝对值：不同任务/库分布不同，0.83 在 A 任务可能很高，在 B 任务可能“刚好”。用验证集调阈值更靠谱。
- 文本 vs 图文通常低于 图像 vs 图文：你的实验也看到这一点——在“同图”前提下，图像会更“主导”；文本是“条件/修饰”。这正是 Visualized‑BGE 的多模态融合特性。 ￼
- 确保归一化一致：库内与查询都做 L2 归一化，和向量索引配置（内积/余弦）一致，避免分数“飘”。